# Generate Prune Select - CISC 867 TensorFlow Reproduction

## Requirements
To install requirements:
```setup
pip install tensorflow version==2.6.0
pip install tensorflow-addons
pip install numpy
pip install scipy
pip install -r orig_requirements.txt
```
## Note
utils.py, Main.py, language_quality.py, were not modified by us - these are from the original repo.
Similaily data/... and utility/.. are requirements from the original.

The GloVe embeddings can be downloaded from https://nlp.stanford.edu/projects/glove/, the 6B pre-trained model was used.

## Code
To execute the first module (generating counterspeech candidates), please execute Final_VAE.ipynb with the appropriate datasets (train_gab.csv and train_reddit.csv) and the glove embedding text file included in a same folder structure. The output is a text document of counterspeech arguments that will be used in modules 2 and 3. 

Please refer to the original authors' README for instructions to run the second and third modules (under "Code"): https://github.com/WanzhengZhu/GPS/blob/master/README.md

## Training
The code that executes training (expected to last around 9 hours on Kaggle for 70 epochs) is present in the main notebook, Final_VAE.ipynb, cell no: 16:

```
from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss')

#Model checkpoints after each epoch 
def create_model_checkpoint(dir, model_name):
    filepath = dir + '/' + model_name + ".h5" #-{epoch:02d}-{decoded_mean:.2f}
    directory = os.path.dirname(filepath)
    try:
        os.stat(directory)
    except:
        os.mkdir(directory)
    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)
    return checkpointer

checkpointer = create_model_checkpoint('models', 'vae_gps')

nb_epoch=70
n_steps = 30000/parameters.batch_size 
for counter in range(nb_epoch):
    print('-------epoch: ',counter,'--------')
    vae.fit(sent_generator(TRAIN_DATA_FILE, parameters.batch_size),steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer, early_stopping],validation_data=(data_1_val, data_1_val))
```

## Evaluation
Please read report for evaluation considerations.


## Model Summary
![alt text](https://github.com/rholford/867_project/blob/main/summary.PNG?raw=true)


## Results
An example of the results generated by the model is show below:
![alt text](https://github.com/rholford/867_project/blob/main/result.PNG?raw=true)

## Files in this repo
We included some non-final implementations to highlight other approaches taken. These are VAE_Text_Generation (Colab baseline).py, training_function.py, vae.py, vae_run.py. All these files include iterative trial and error efforts towards the final submission. 
The saved model (.h5 format) can be accessed here: https://drive.google.com/file/d/1Zbtag_JezIdmdtwNq3rFBAWW6qUZQkgK/view?usp=sharing

## Overleaf link for the final report
https://www.overleaf.com/read/gssrgmyfdkqm
