{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Necessary import of libraries\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.advanced_activations import ELU\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nfrom keras.models import Model\nfrom scipy import spatial\nfrom keras import backend\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport codecs\nimport csv\nimport json\nimport ast\nimport random","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638706465008,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"Bu5JfkHlTAzA","execution":{"iopub.status.busy":"2021-12-07T03:40:03.332168Z","iopub.execute_input":"2021-12-07T03:40:03.334071Z","iopub.status.idle":"2021-12-07T03:40:08.305518Z","shell.execute_reply.started":"2021-12-07T03:40:03.334024Z","shell.execute_reply":"2021-12-07T03:40:08.304658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing datasets for simpler input format\n\ndef ExtractData(datasetName):\n    \"\"\"\n    Returns training and testing dataset from dataset name ('gab, 'reddit', 'conan')\n    \"\"\"\n    #Training data percent\n    pct = 0.80\n    if datasetName in ['reddit', 'gab']:\n        dataFile = open ('../input/gab-data/' + datasetName + '.csv', 'r', encoding='utf-8')\n        hateSpeechBlob = []\n        hsIdx = []\n        responseBlob = []\n        \n        reader = csv.DictReader(dataFile)\n        for row in reader:\n            x = row['text']\n            y = row['hate_speech_idx']\n            z = row['response']\n            if y == 'n/a':\n                continue\n            hateSpeechBlob.append(x)\n            hsIdx.append(y)\n            responseBlob.append(z)\n        \n        hateCount = 0\n        for item in hsIdx:\n            for i in item.strip('[]').split(', '):\n                hateCount += 1\n\n        hateSpeech, counterSpeech = [], []\n        lineNumber = 0\n        for hs, idx, cs in zip(hateSpeechBlob, hsIdx, responseBlob):\n            hs = hs.strip().split('\\n')\n            for i in idx.strip('[]').split(', '):\n                try:\n                    hateSpeech.append('. '.join(hs[int(i) - 1].split('. ')[1:]).strip('\\t'))\n                except:\n                    continue\n                    #Note this is because there is an error in the data that throws out of bounds\n                temp = []\n                for j in splitResponse(cs):\n                    if j.lower() == 'n/a':\n                        continue\n                    temp.append(j)\n                counterSpeech.append(temp)\n                lineNumber += 1\n        hateCount = len(hateSpeech)\n                \n\n    elif datasetName == 'conan':\n        dataFile = open ('./data/conan/CONAN.json', 'r')\n        fileText = []\n        for line in dataFile:\n            \n            fileText.append(json.loads(line))\n\n        enText =  []\n        for item in fileText[0]['conan']:\n            if (item['cn_id'][:2] == 'EN'):\n                enText.append(item)\n\n        hateSpeech = []\n        counterSpeech =[] \n        for item in enText:\n            hateSpeech.append(item['hateSpeech'].strip())\n            counterSpeech.append([item['counterSpeech'].strip()])\n        hateCount = len(hateSpeech)\n        dataFile.close()\n       \n    randomIndex = []\n    for num in range(hateCount):\n        randomIndex.append(num) \n    random.shuffle(randomIndex)\n    trainIndex = sorted(randomIndex[:int(pct*len(randomIndex))])\n    trainHate = []\n    for i in range(hateCount):\n        if (i in trainIndex):\n            trainHate.append(hateSpeech[i])\n    trainCounter = []\n    for i in range(hateCount):\n        if (i in trainIndex):\n            trainCounter.append(counterSpeech[i])\n\n    testHate = []\n    for i in range(hateCount):\n        if (i not in trainIndex):\n            testHate.append(hateSpeech[i])\n    testCounter = []\n    for i in range(hateCount):\n        if (i not in trainIndex):\n            testCounter.append(counterSpeech[i])\n    trainCounter = flatten(trainCounter)\n    testCounter = flatten(testCounter)\n    \n    #Flattening \n    return trainHate, trainCounter, testHate, testCounter\n\n#Flatten counter speech\ndef flatten(lst):\n    lstOut = []\n    for subLst in lst:\n        for val in subLst:\n            lstOut.append(val)\n    return lstOut\n                   \n\n#Helper function for csvs\ndef splitResponse(strResp):\n    result = ast.literal_eval(strResp)\n    #print(result)\n    retVal = []\n    for item in result:\n        retVal.append(item)\n    return retVal\n\n\ndef main():\n    #a, b, c, d = ExtractData('conan')\n    #a, b, c, d= ExtractData('reddit')\n    a,b,c,d = ExtractData('gab')\nif __name__ == \"__main__\":\n    main()","metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1638706640952,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"b_jlJfmcAnKU","execution":{"iopub.status.busy":"2021-12-07T03:40:08.307112Z","iopub.execute_input":"2021-12-07T03:40:08.307397Z","iopub.status.idle":"2021-12-07T03:40:12.770779Z","shell.execute_reply.started":"2021-12-07T03:40:08.307361Z","shell.execute_reply":"2021-12-07T03:40:12.76998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train and test samples for counterspeech and hatespeech arguments\ntrainhate, traincounter, testhate, testcounter = ExtractData('gab')","metadata":{"executionInfo":{"elapsed":274,"status":"aborted","timestamp":1638706465711,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"uMXkEB0YCYIc","execution":{"iopub.status.busy":"2021-12-07T03:40:12.774879Z","iopub.execute_input":"2021-12-07T03:40:12.775116Z","iopub.status.idle":"2021-12-07T03:40:17.511718Z","shell.execute_reply.started":"2021-12-07T03:40:12.775089Z","shell.execute_reply":"2021-12-07T03:40:17.510959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(traincounter))\nprint(len(testcounter))","metadata":{"executionInfo":{"elapsed":275,"status":"aborted","timestamp":1638706465712,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"2PBOhgDFCxjp","execution":{"iopub.status.busy":"2021-12-07T03:40:17.513053Z","iopub.execute_input":"2021-12-07T03:40:17.513291Z","iopub.status.idle":"2021-12-07T03:40:17.519304Z","shell.execute_reply.started":"2021-12-07T03:40:17.51326Z","shell.execute_reply":"2021-12-07T03:40:17.518697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a train data file\n\ntrain_gab = pd.DataFrame()\ntrain_gab['counterspeech'] = traincounter\ntrain_gab.head()","metadata":{"executionInfo":{"elapsed":276,"status":"aborted","timestamp":1638706465713,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"PUrVYnUWEcKh","execution":{"iopub.status.busy":"2021-12-07T03:40:17.520433Z","iopub.execute_input":"2021-12-07T03:40:17.520824Z","iopub.status.idle":"2021-12-07T03:40:17.577365Z","shell.execute_reply.started":"2021-12-07T03:40:17.520789Z","shell.execute_reply":"2021-12-07T03:40:17.576708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading an individual dataset and glove embeddings\n\nTRAIN_DATA_FILE = '../input/gpskaggle/train_gab.csv'\nGLOVE_EMBEDDING = '../input/gpskaggle/glove.6B.50d.txt'\n\n#Small set of hyperparameters, they match the ones passed as arguments later on\nVALIDATION_SPLIT = 0.2\nMAX_SEQUENCE_LENGTH = 15\nMAX_NB_WORDS = 12000\nEMBEDDING_DIM = 50\n\ntexts = [] \nwith codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        texts.append(values[1])\nprint('Found %s texts in train.csv' % len(texts))","metadata":{"executionInfo":{"elapsed":1271,"status":"ok","timestamp":1638706653276,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"CqTS7mvgUZ8A","outputId":"24d25182-8471-4105-eb9b-8ada40477651","execution":{"iopub.status.busy":"2021-12-07T03:40:17.578615Z","iopub.execute_input":"2021-12-07T03:40:17.578845Z","iopub.status.idle":"2021-12-07T03:40:17.890162Z","shell.execute_reply.started":"2021-12-07T03:40:17.578814Z","shell.execute_reply":"2021-12-07T03:40:17.889393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[1]","metadata":{"executionInfo":{"elapsed":377,"status":"ok","timestamp":1638706656437,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"rIpA_iH7KnV_","outputId":"aa0113c5-193e-4d7f-f0a7-5de3bfa09a49","execution":{"iopub.status.busy":"2021-12-07T03:40:17.891429Z","iopub.execute_input":"2021-12-07T03:40:17.89182Z","iopub.status.idle":"2021-12-07T03:40:17.897518Z","shell.execute_reply.started":"2021-12-07T03:40:17.891782Z","shell.execute_reply":"2021-12-07T03:40:17.896842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenizing sentences \ntokenizer = Tokenizer(MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nword_index = tokenizer.word_index #The dict values start from 1 so this is fine with zeropadding\nindex2word = {v: k for k, v in word_index.items()}\nprint('Found %s unique tokens' % len(word_index))\nsequences = tokenizer.texts_to_sequences(texts)\ndata_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data_1.shape)\nNB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\ndata_1_val = data_1[27291:33291] #Select 6000 sentences as validation data (0.2)\ndata_train = data_1[:27291]","metadata":{"executionInfo":{"elapsed":1487,"status":"ok","timestamp":1638706659082,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"4q9DcbSPVgKs","outputId":"fda7c37f-2f16-461f-bc3d-70f36c0a126c","execution":{"iopub.status.busy":"2021-12-07T03:40:17.898929Z","iopub.execute_input":"2021-12-07T03:40:17.899581Z","iopub.status.idle":"2021-12-07T03:40:19.223477Z","shell.execute_reply.started":"2021-12-07T03:40:17.899544Z","shell.execute_reply":"2021-12-07T03:40:19.222611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.shape","metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1638706663027,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"0jQrgpAtIOYo","outputId":"5c7bd1a6-adea-41f8-e22e-bc79eb24cd54","execution":{"iopub.status.busy":"2021-12-07T03:40:19.226733Z","iopub.execute_input":"2021-12-07T03:40:19.226959Z","iopub.status.idle":"2021-12-07T03:40:19.23483Z","shell.execute_reply.started":"2021-12-07T03:40:19.226932Z","shell.execute_reply":"2021-12-07T03:40:19.233962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_1_val.shape","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638706664013,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"joiYlaFTRptN","outputId":"e8a9a0cb-a90f-4d27-98c6-d793e711901c","execution":{"iopub.status.busy":"2021-12-07T03:40:19.236488Z","iopub.execute_input":"2021-12-07T03:40:19.23676Z","iopub.status.idle":"2021-12-07T03:40:19.242623Z","shell.execute_reply.started":"2021-12-07T03:40:19.236723Z","shell.execute_reply":"2021-12-07T03:40:19.241939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Small helper function to pass sentences in sequential batches for better model fit\n\ndef sent_generator(TRAIN_DATA_FILE, chunksize):\n    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n    for df in reader:\n        \n        val = df.iloc[:,1:2].values.tolist()\n        #print(val)\n        flat3 = [item for sublist in val for item in sublist]\n        #print(flat3)\n        flat = [str(item) for sublist in val for item in sublist]\n        #print(flat)\n        texts = [] \n        texts.extend(flat[:])\n        \n        sequences = tokenizer.texts_to_sequences(texts)\n        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n        yield [data_train, data_train]","metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1638706665507,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"Zeb3f5E_Vi7P","execution":{"iopub.status.busy":"2021-12-07T03:40:19.243933Z","iopub.execute_input":"2021-12-07T03:40:19.244489Z","iopub.status.idle":"2021-12-07T03:40:19.252536Z","shell.execute_reply.started":"2021-12-07T03:40:19.244451Z","shell.execute_reply":"2021-12-07T03:40:19.251786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using glove to generate embeddings for tokenized sentences\n\nembeddings_index = {}\nf = open(GLOVE_EMBEDDING, encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\nglove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i < NB_WORDS:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be the word embedding of 'unk'.\n            glove_embedding_matrix[i] = embedding_vector\n        else:\n            glove_embedding_matrix[i] = embeddings_index.get('unk')\nprint('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))","metadata":{"executionInfo":{"elapsed":8557,"status":"ok","timestamp":1638706679776,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"2hL_ooj3Vtl3","outputId":"f416ecc0-dd4d-4b48-a3d8-df313becd9cb","execution":{"iopub.status.busy":"2021-12-07T03:40:19.253876Z","iopub.execute_input":"2021-12-07T03:40:19.254142Z","iopub.status.idle":"2021-12-07T03:40:28.715139Z","shell.execute_reply.started":"2021-12-07T03:40:19.254108Z","shell.execute_reply":"2021-12-07T03:40:28.713713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparameters passed as arguments\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--batch_size', type=int, default=8) \nparser.add_argument('--n_vocab', type=int, default=12000)\nparser.add_argument('--epochs', type=int, default=100) # Changed to 100 from 1000\nparser.add_argument('--n_hidden_G', type=int, default=512)\nparser.add_argument('--n_layers_G', type=int, default=2)\nparser.add_argument('--n_hidden_E', type=int, default=512)\nparser.add_argument('--n_layers_E', type=int, default=1)\nparser.add_argument('--n_z', type=int, default=100)\nparser.add_argument('--word_dropout', type=float, default=0.5)\nparser.add_argument('--rec_coef', type=float, default=7)\nparser.add_argument('--lr', type=float, default=0.00001)\nparser.add_argument('--gpu', type=int, default=0)\nparser.add_argument('--n_highway_layers', type=int, default=2)\nparser.add_argument('--n_embed', type=int, default=50) # Same as EMBEDDING_DIM\nparser.add_argument('--out_num', type=int, default=30000)\nparser.add_argument('--unk_token', type=str, default=\"<unk>\")\nparser.add_argument('--pad_token', type=str, default=\"<pad>\")\nparser.add_argument('--start_token', type=str, default=\"<sos>\")\nparser.add_argument('--end_token', type=str, default=\"<eos>\")\nparser.add_argument('--dataset', type=str, default=\"reddit\")\nparser.add_argument('--training', action='store_true')\nparser.add_argument('--resume_training', action='store_true')\n\n\nparameters, unknown = parser.parse_known_args()","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638706680580,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"UnigvYzBVwk_","execution":{"iopub.status.busy":"2021-12-07T03:40:28.716499Z","iopub.execute_input":"2021-12-07T03:40:28.716763Z","iopub.status.idle":"2021-12-07T03:40:28.730478Z","shell.execute_reply.started":"2021-12-07T03:40:28.716728Z","shell.execute_reply":"2021-12-07T03:40:28.729648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.framework.ops import disable_eager_execution\ndisable_eager_execution()\n \n#VAE model definition \n\n#Placeholder loss\ndef zero_loss(y, y_hat):\n    return backend.zeros_like(y_hat)\n\nx = Input(batch_shape=(None,15))\nx_embed = Embedding(NB_WORDS,parameters.n_embed,weights=[glove_embedding_matrix],input_length=15,trainable=False)(x)\nh = Bidirectional(LSTM(parameters.n_hidden_G,return_sequences=False,recurrent_dropout=0.2),merge_mode=\"concat\")(x_embed)\nh = Dropout(0.2)(h)\nmu = Dense(parameters.n_z)(h)\nlog_var = Dense(parameters.n_z)(h)\n\ndef sample(args):\n    print (\"entered sample function\")\n    mu, log_var = args\n    eps = tf.random.normal(shape=(parameters.batch_size,parameters.n_z),mean=0,stddev=1)\n    return mu + tf.exp(0.5*log_var)*eps\n\nz = Lambda(sample,output_shape=(parameters.n_z,))([mu,log_var])\nrepeat_vector = RepeatVector(15)\ndecoder_h = LSTM(parameters.n_hidden_E,return_sequences=True,recurrent_dropout=0.2)\ndecoder_mu = TimeDistributed(Dense(parameters.n_vocab,activation='linear'))\ndecoded_h = decoder_h(repeat_vector(z))\ndecoded_mu = decoder_mu(decoded_h)\nprint (\"functioning\")\nlogits = tf.constant(np.random.randn(parameters.batch_size, 15, parameters.n_vocab), tf.float32)\ntargets = tf.constant(np.random.randint(parameters.n_vocab, size=(parameters.batch_size, 15)), tf.int32)\nproj_w = tf.constant(np.random.randn(parameters.n_vocab, parameters.n_vocab), tf.float32)\nproj_b = tf.constant(np.zeros(parameters.n_vocab), tf.float32)\n\ndef sample_loss(labels, logits):\n    print (\"sample loss function entered\")\n    labels = labels.reshape(tf.cast(labels,tf.int64),[-1,1])\n    logits = tf.cast(logits, tf.float32)\n    return tf.cast(tf.nn_sampled_softmax_loss(\n        proj_w, proj_b, labels, logits, num_sampled=500,num_classes = parameters.n_vocab\n    ),tf.int32)\n\nsoftmax_loss = sample_loss\n\n\n#VAE layers, including custom loss\nclass VAELayer(Layer):\n    def __init__(self, **kwargs):\n        super(VAELayer,self).__init__(**kwargs)\n        self.target_weights = tf.constant(np.ones((parameters.batch_size, 15)),tf.float32) \n\n    def vae_loss(self, x, decoded_mu):\n        labels = tf.cast(x, tf.int32)\n        recreation_loss = backend.sum(tfa.seq2seq.sequence_loss(\n            decoded_mu,labels,weights=self.target_weights,average_across_timesteps=False,\n            average_across_batch=False),axis=-1)\n\n        kl_loss = -0.5*backend.sum(1+log_var-backend.square(mu)-backend.exp(log_var))\n        return backend.mean(kl_loss + recreation_loss)\n\n    def call(self, inputs):\n        print (\"call function entered\")\n        x = inputs[0]\n        decoded_mu = inputs[1]\n        loss = self.vae_loss(x,decoded_mu)\n        self.add_loss(loss,inputs=inputs)\n        return backend.zeros_like(x)\n\nloss_layer = VAELayer()([x,decoded_mu])\nvae = Model(x,[loss_layer])\nopt = tf.keras.optimizers.Adam(lr=parameters.lr) #Adam optimizer as per GPS\nvae.compile(optimizer=opt,loss=[zero_loss])\nvae.summary()","metadata":{"executionInfo":{"elapsed":13666,"status":"ok","timestamp":1638706710865,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"2Lbj7Dv-WWVU","outputId":"ad23710e-3373-4175-84f4-c49860f7488d","execution":{"iopub.status.busy":"2021-12-07T03:40:28.731965Z","iopub.execute_input":"2021-12-07T03:40:28.732494Z","iopub.status.idle":"2021-12-07T03:40:40.727779Z","shell.execute_reply.started":"2021-12-07T03:40:28.732401Z","shell.execute_reply":"2021-12-07T03:40:40.726968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# steps_per_epoch = int( np.ceil(train_gab.shape[0] / parameters.batch_size) )\nprint (11000/parameters.batch_size)\nprint (11000//parameters.batch_size)","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638706711693,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"Q9P0A7DoVmcz","outputId":"8ba699bd-cafc-42b2-d9a3-e7eef45562a8","execution":{"iopub.status.busy":"2021-12-07T03:40:40.72912Z","iopub.execute_input":"2021-12-07T03:40:40.729437Z","iopub.status.idle":"2021-12-07T03:40:40.73648Z","shell.execute_reply.started":"2021-12-07T03:40:40.7294Z","shell.execute_reply":"2021-12-07T03:40:40.735824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.framework.ops import disable_eager_execution\ndisable_eager_execution()\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss')\n\n#Model checkpoints after each epoch \ndef create_model_checkpoint(dir, model_name):\n    filepath = dir + '/' + model_name + \".h5\" #-{epoch:02d}-{decoded_mean:.2f}\n    directory = os.path.dirname(filepath)\n    try:\n        os.stat(directory)\n    except:\n        os.mkdir(directory)\n    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n    return checkpointer\n\ncheckpointer = create_model_checkpoint('models', 'vae_gps')\n\nnb_epoch=70\nn_steps = 30000/parameters.batch_size \nfor counter in range(nb_epoch):\n    print('-------epoch: ',counter,'--------')\n    vae.fit(sent_generator(TRAIN_DATA_FILE, parameters.batch_size),steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer, early_stopping],validation_data=(data_1_val, data_1_val))","metadata":{"id":"VZc7DBD1YP1z","outputId":"ff5327e9-3315-4a13-e2a6-d7badb29a5ad","execution":{"iopub.status.busy":"2021-12-07T03:40:40.737788Z","iopub.execute_input":"2021-12-07T03:40:40.738199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save final model as .h5 file\n\nvae.save('./vae_lstm800k32dim96hid.h5')","metadata":{"executionInfo":{"elapsed":374,"status":"ok","timestamp":1638682818889,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"2uu-RY9OS6_S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build a model to project sentences on the latent space\nencoder = Model(x, mu)\n\n#Build a generator that can sample sentences from the learned distribution\ndecoder_input = Input(shape=(parameters.n_z,))\n_h_decoded = decoder_h(repeat_vector(decoder_input))\n_x_decoded_mean = decoder_mu(_h_decoded)\n_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\ngenerator = Model(decoder_input, _x_decoded_mean)","metadata":{"executionInfo":{"elapsed":789,"status":"ok","timestamp":1638682828712,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"a9of9Hv7gRJm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating for validation sentences\n\nindex2word = {v: k for k, v in word_index.items()}\nsent_encoded = encoder.predict(data_1_val, batch_size = 8)\nx_test_reconstructed = generator.predict(sent_encoded)\n                                         \nsent_idx = 672\nreconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n#np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\nword_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\nprint(word_list)\noriginal_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\nprint(original_sent)","metadata":{"executionInfo":{"elapsed":26344,"status":"ok","timestamp":1638682861425,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"pEFl3ZBySoPN","outputId":"24ab431e-0d96-409d-dabe-0645596f630e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to parse a sentence\ndef sent_parse(sentence, mat_shape):\n    sequence = tokenizer.texts_to_sequences(sentence)\n    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n    return padded_sent #[padded_sent, sent_one_hot]\n\n\n# Input: original dimension sentence vector\n# Output: sentence text\ndef print_latent_sentence(sent_vect):\n    \n    sent_vect = np.reshape(sent_vect,[1,parameters.n_z])\n    #print(sent_vect.shape) \n    sent_reconstructed = generator.predict(sent_vect)\n    #print(sent_reconstructed.shape)\n    sent_reconstructed = np.reshape(sent_reconstructed,[MAX_SEQUENCE_LENGTH,parameters.n_vocab])\n    #print(sent_reconstructed)\n    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n    #print(reconstructed_indexes)\n    #np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n    #np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n\n    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n    w_list = [w for w in word_list if w]\n    return ' '.join(w_list)\n    #print(word_list)","metadata":{"executionInfo":{"elapsed":86,"status":"ok","timestamp":1638682875497,"user":{"displayName":"Sachin VS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4k5PerA99dRJgZOBPPOfmYFQ8rIY7BI6pbrR4MA=s64","userId":"09169179031388709149"},"user_tz":300},"id":"uPiCaMNEgo3Z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looping a subset of data for generation\n\nut = []\ndef convlist(lst):\n    return [[el] for el in lst]\n\ninplist = convlist(testcounter)\nfor item in inplist:\n    mysent = sent_parse(item, [15])\n    mysent_encoded = encoder.predict(mysent, batch_size = 16)\n    x = print_latent_sentence(mysent_encoded)\n    #print (type(x))\n    out.append(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing with a specific instance\n\nsentence1=['Using words that insult one group while defending another group doesnt come across as helpful.']\nmysent = sent_parse(sentence1, [15])\nmysent_encoded = encoder.predict(mysent, batch_size = 16)\nx = print_latent_sentence(mysent_encoded)\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Writing generated counterspeech sentences into a text file\nwith open('./gab.txt', 'w') as f:\n    f.write('\\r\\n'.join(out))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}